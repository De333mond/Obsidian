#Polykek/cheats/TOI #искусственный-интеллект 

## Понятие машинного обучения.

Несмотря на полувековой срок существования понятий Машинного обучения, Нейронный сетей, Искусственного интеллекта, существует несколько разных точек зрения на классификацию их относительно друг друга. В данной лекции не будет рассматриваться данный вопрос, а речь пойдет об одной из областей интеллектуального программирования – машинном обучении.

Машинное обучение — класс методов искусственного интеллекта (ИИ), позволяющих компьютерам обучаться на основе данных (обычно в ходе решения узкоспециализированных задач) без явного программирования. Термин машинное обучение был введен Артуром Сэмюэлем, легендой в области ИИ, еще в 1959 году, однако до конца XX века лишь немногим проектам удалось достигнуть коммерческого успеха в области машинного обучения, которая оставалась не более чем нишей для академических исследований.

Несмотря на сильное развитие в последние годы противоборствующей фракции – Нейронных сетей, машинное обучение не утратило актуальность. Оно активно применяется в задачах классификации и регрессии, где наблюдается недостаток данных или ограниченные вычислительные мощности. Например, задачи классификации спама или таргетированная реклама.

## Применяемые методы машинного обучения.

Машинное обучение, как и аналогичные интеллектуальные технологии способно решать задачи, разделяемые на два крупных кластера: 
1. Задачи регрессии. В таких задачах от ИИ требуется сгенерировать ранее не существовавший ответ. Он может быть любого формата, начиная с цифр, заканчивая текстом или изображением. 
2. Задачи классификации. Это принципиально другой кластер задач. ИИ необходимо отнести поступающие данные к одному из заранее известных кластеров, либо разделить данные на несколько неизвестных кластеров.

В подходам к обучению можно выделить две основные методологии: обучение с учителем (supervised learning), или контролируемое обучение, и обучение без учителя (unsupervised learning), или неконтролируемое обучение (самообучение). Есть еще множество методик, которые являются своеобразными мостиками между ними.

Обучение с учителем. Данный метод подразумевает наличие заранее подготовленных и размеченных данных. На вход ИИ подаются подготовленные данные которые необходимо научиться распознавать и набор правильных ответов к ним. Таким образом ИИ учится выделять в данных зависимости и проверяет себя по правильным ответам.

### Линейная регрессия

Простейшим из всех алгоритмов является линейная регрессия, в которой используется модель, предполагающая линейное соотношение между входными переменными (X) и единственным выходным значением (у). Если соотношение между входами и выходом действительно линейное, а между входными переменными отсутствует значительная корреляция (ситуация, известная как коллинеарность), то линейная регрессия станет неплохим выбором. Если же истинное соотношение носит более сложный или нелинейный характер, то использование линейной регрессии приведет к недообучению

#### Сильные стороны
Линейная регрессия отличается простотой, интерпретируемостью и несклонностью к переобучению, поскольку она не в состоянии моделировать чрезмерно сложные отношения. Она великолепно подходит для тех случаев, когда соотношение между входными и выходными переменными в основном является линейным.

#### Слабые стороны
Линейная регрессия будет недообучаться на данных, если соотношение между входными и выходными переменными нелинейное

#### Применение 
Поскольку вес человека в целом линейно зависит от его роста, линейная регрессия будет хорошо предсказывать вес человека, используя его рост в качестве входных данных, и наоборот.

### Логистическая регрессия
Простейшим алгоритмом классификации является логистическая регрессия, которая также относится к линейным методам, но ее предсказания преобразуются с помощью логистической функции. Выходом такого преобразования являются вероятности классов — иначе говоря, вероятности принадлежности образца к тому или иному классу, причем сумма всех вероятностей по всем образцам должна равняться единице. Затем каждому примеру приписывается класс, вероятность принадлежности к которому максимальная.

#### Сильные стороны 
Как и линейная регрессия, логистическая регрессия отличается простотой и интерпретируемостью. Если классы, которые мы пытаемся предсказать, не перекрываются и линейно разделимы, то логистическая регрессия — отличный выбор. 
#### Слабые стороны 
Логистическая регрессия не работает в случае линейно неразделимых классов. 
#### Применение 
Если классы — например, рост ребенка и рост взрослого человека — в целом не перекрываются, то логистическая регрессия даст хорошие результаты.

## Методы на основе соседства точек
Другую группу очень простых алгоритмов образуют методы, основанные на понятии соседства точек (neighborhood). Методы этой группы — ленивые ученики, поскольку в данном случае обучение разметке новых точек основывается на их близости к уже размеченным точкам. В отличие от линейной или логистической регрессии модели, основанные на соседстве точек, не обучаются предсказанию меток для новых точек. Вместо этого они предсказывают метки для новых точек, исключительно исходя из того, насколько новые точки удалены от существующих размеченных точек.

### Метод K-ближайших соседей
Наиболее популярный метод этой категории — k-ближайших соседей (k-nearest neighbors — KNN). Для пометки каждой новой точки алгоритм осуществляет поиск к ближайших соседних помеченных точек (где к — целое число) и наделяет уже размеченных соседей правом голоса при принятии решения относительно того, какую метку следует присвоить новой точке. В качестве меры удаленности новой точки от ближайших соседей в KNN по умолчанию используется евклидово расстояние. 

От выбора значения к зависит очень многое. Если оно установлено слишком низким, то метод приобретает повышенную гибкость, очерчивая границы, учитывающие множество нюансов, и переобучаясь на данных. Если же для к выбрано слишком большое значение, то метод теряет гибкость, очерчивая слишком грубые границы и потенциально недообучаясь на данных.

#### Сильные стороны 
KNN, в отличие от линейных методов, обладает значительной гибкостью и приспособлен для обучения более сложным нелинейным соотношениям. При этом он сохраняет простоту и интерпретируемость. 
#### Слабые стороны 
KNN плохо работает в тех случаях, когда количество наблюдений и признаков достигает критической величины. В условиях заполненного большим количеством точек многомерного пространства метод становится неэффективным с вычислительной точки зрения, поскольку для предсказания признаков приходится рассчитывать расстояния от новой точки до уже размеченных многочисленных соседних точек. Он не позволяет использовать эффективную модель с уменьшенным количеством параметров для генерирования необходимых предсказаний. Кроме того, метод весьма чувствителен к выбору параметра к. При слишком низких значениях к метод может переобучаться, а при слишком высоких — недообучаться. 
#### Применение 
KNN широко применяется в рекомендательных системах, например тех, которые используются для прогнозирования пользовательских предпочтений: фильмов (Netflix), музыки (Spotify), друзей (Facebook), фотографий (Instagram), поисковых запросов (Google) и потребительских товаров (Amazon). В частности, метод может оказаться полезным при предсказании предпочтений отдельного пользователя на основании известных предпочтений пользователей с аналогичными вкусами (так называемая коллаборативная фильтрация) или на основании собственных предпочтений пользователя в прошлом (так называемая фильтрация по содержимому).

### Методы на основе деревьев решений
Вместо того чтобы использовать линейный метод, мы можем построить дерево решений, в котором все примеры сегментируются, или стратифицируются, по отдельным областям на основании имеющихся меток. По завершении сегментирования каждая область соответствует определенному классу меток (для задач классификации) или диапазону предсказанных значений (для задач регрессии). Этот процесс аналогичен тому, как если бы мы поручили ИИ автоматически создать правила, ориентированные на получение наилучших решений или предсказаний. 

#### Одиночное дерево решений 
Простейший вариант — метод одиночного дерева решений, в котором ИИ выполняет один проход по тренировочным данным, создает правила для сегментирования данных на основании имеющихся меток и использует результирующее дерево решений для создания предсказаний в отношении не представленных ранее данных, включенных в валидационный или тестовый набор. Однако одиночное дерево решений обычно плохо обобщает то, чему обучилось в процессе тренировки, на незнакомые примеры ввиду переобучения на одной-единственной тренировочной итерации. 

#### Случайные леса 
Мы можем дополнительно ослабить эффекты переобучения, семплируя не только примеры, но и сами предикторы. В методе случайных лесов мы отбираем несколько случайных выборок примеров из тренировочных данных в каждом дереве решений мы создаем расщепление, основанное не на всех предикторах, а на случайной выборке предикторов. Количество предикторов, устанавливаемых для каждого дерева, обычно выбирается равным квадратному корню из общего их количества. 

Семплируя предикторы описанным способом, алгоритм случайных лесов создает деревья, еще в меньшей степени скоррелированные между собой, тем самым ослабляя эффекты переобучения и уменьшая ошибку обобщения. 
#### Сильные стороны 
Методы на основе деревьев решений относятся к числу наиболее производительных алгоритмов обучения с учителем, предназначенных для решения задач прогнозирования. Эти методы способны выявлять сложные соотношения в данных путем обучения многим простым правилам, по одному правилу за раз. Кроме того, они способны обрабатывать отсутствующие данные и категориальные признаки. 
#### Слабые стороны 
Методы на основе деревьев решений с трудом поддаются интерпретации, особенно в тех случаях, когда для получения надежных предсказаний требуется использовать большое количество признаков. Кроме того, по мере увеличения количества признаков производительность также становится проблемой. 
#### Применение 
Случайные леса отлично подходят для решения задач прогнозирования. Обучение без учителя. В данном подходе к обучению отсутствуют подготовленные и размеченные данные. ИИ необходимо научиться искать закономерности данных и на основе них предоставлять результат. Данные методы хоть и обучаются дольше, но незаменимы, когда нет возможности разметить данные, либо служат для подготовки данных для обучения с учителем.

### Метод K-средних
Чтобы успешно справиться с кластеризацией, мы должны идентифицировать отдельные группы, примеры в пределах которых схожи между собой, но отличаются от примеров, относящихся к другим группам. Одним из таких алгоритмов является кластеризация методом к-средних (K-means clustering). В данном алгоритме мы задаем желаемое количество кластеров, и алгоритм относит каждый пример ровно к одному из этих к кластеров. Алгоритм оптимизирует группирование, минимизируя внутрикластерную вариацию (называемую инерцией) так, чтобы сумма внутрикластерных вариаций по всем к кластерам была как можно меньшей.

С целью ускорения процесса кластеризации метод K-средних случайным образом относит каждое наблюдение к одному из к кластеров, а затем переназначает наблюдения для минимизации евклидовых расстояний между каждым наблюдением и центральной точкой кластера, или центроидом. Как следствие, различные запуски алгоритма K-средних — каждый со своей начальной точкой — будут приводить к несколько различающимся отнесениям наблюдений к тем или иным кластерам. Из этой серии различных запусков мы выбираем тот, который характеризуется наилучшим разделением, дающим наименьшую общую сумму внутрикластерных вариаций по всем к кластерам.

### Иерархическая кластеризация 
Альтернативой обычной кластеризации служит так называемая иерархическая кластеризация, не требующая предварительного задания количества кластеров. В одном из вариантов иерархической кластеризации, известном как агломеративная кластеризация, применяется кластеризация на основе деревьев и создается так называемая дендрограмма. Последняя графически отображается в виде перевернутого дерева, в котором листья находятся внизу, а ствол дерева — вверху. 

Самые нижние листья — это индивидуальные примеры, содержащиеся в наборе данных. Затем, по мере перемещения вверх по перевернутому дереву, иерархическая кластеризация объединяет листья на основании их взаимного сходства. В первую очередь объединяются наиболее схожие примеры (или группы примеров), в последнюю очередь — наименее схожие. В конечном счете описанный итеративный процесс приводит к тому, что все примеры оказываются связанными, образуя единый ствол дерева. 

Такое графическое представление весьма информативно. Как только алгоритм иерархической кластеризации завершит свою работу, мы сможем проанализировать дендрограмму и определить, в каком месте мы хотим обрезать дерево. Чем ниже линия обреза, тем больше останется индивидуальных ветвей (т.е. кластеров). Если мы хотим уменьшить количество кластеров, то линия обреза должна располагаться высоко на дендрограмме, ближе к стволу, находящемуся на самом верху перевернутого дерева. Размещение линии обреза аналогично выбору количества к кластеров в алгоритме кластеризации методом K-средних.

## Понятие Регрессии

Технически регрессия – это общее название задач, решаемых машинным обучением, когда алгоритму необходимо спрогнозировать некоторую выходную величину на основе поступающих данных. Это становится возможным в случае, если между данными четко просматривается какой-либо вид зависимостей. 

Примеры видов зависимости переменных показаны на рис. 2. Далее каждый вид зависимостей будет рассмотрен подробнее.

![[Pasted image 20240611075520.png]]

На панели А значения переменной Y почти линейно возрастают с увеличением переменной X. Примером этой зависимости является связь между объемом продаж и площадью торгового помещения.

![[Pasted image 20240611075531.png]]

Панель Б является примером отрицательной линейной зависимости. Если переменная X возрастает, переменная Y в целом убывает. Примером этой зависимости является связь между стоимостью конкретного товара и объемом продаж.

![[Pasted image 20240611075540.png]]

На панели В показан набор данных, в котором переменные X и Y практически не зависят друг от друга. Каждому значению переменной X соответствуют как большие, так и малые значения переменной Y

![[Pasted image 20240611075552.png]]

Данные, приведенные на панели Г, демонстрируют криволинейную зависимость между переменными X и Y. Значения переменной Y возрастают при увеличении переменной X, однако скорость роста после определенных значений переменной X падает. Примером положительной криволинейной зависимости является связь между возрастом и стоимостью обслуживания автомобилей. По мере старения машины стоимость ее обслуживания сначала резко возрастает, однако после определенного уровня стабилизируется.

![[Pasted image 20240611075606.png]]

Панель Д демонстрирует параболическую У-образную форму зависимости между переменными X и Y. По мере увеличения значений переменной X значения переменной Y сначала убывают, а затем возрастают. Примером такой зависимости является связь между количеством ошибок, совершенных за час работы, и количеством отработанных часов. Сначала работник осваивается и делает много ошибок, потом привыкает, и количество ошибок уменьшается, однако после определенного момента он начинает чувствовать усталость, и число ошибок увеличивается.

![[Pasted image 20240611075627.png]]

На панели Е показана экспоненциальная зависимость между переменными X и Y. В этом случае переменная Y сначала очень быстро убывает при возрастании переменной X, однако скорость этого убывания постепенно падает. Например, стоимость автомобиля при перепродаже экспоненциально зависит от его возраста. Если перепродавать автомобиль в течение первого года, его цена резко падает, однако впоследствии ее падение постепенно замедляется. Пример простейшей (линейной) зависимости показан на рис. 3.

![[Pasted image 20240611075640.png]]

Простая линейная регрессия: 
`𝑌𝑖 = 𝑏0 + 𝑏1 * 𝑋1` 
Где 
𝑏 -сдвиг (длина отрезка, отсекаемого на координатной оси прямой Y),  
𝑏1 -наклон прямой Y.

## Метод наименьших квадратов.

Метод наименьших квадратов (МНК) — математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений некоторых функций от искомых переменных. МНК является одним из базовых методов регрессионного анализа для оценки неизвестных параметров регрессионных моделей по выборочным данным.

Для получения уравнения линейной регрессии может быть использован метод наименьших квадратов.

![[Pasted image 20240611075741.png]]

![[Pasted image 20240611075755.png]]

Здесь R2 - Коэффициент детерминации показывает долю вариации результативного признака Y под влиянием факторного признака X. При отсутствии связи эмпирический коэффициент детерминации равен нулю, а при функциональной связи — единице. Применение уравнения регрессии допустимо только для интерполяции, но не применимо для экстраполяции

Пример применения Метода наименьших квадратов: Методом наименьших квадратов для данных, представленных в таблице, найти линейную зависимость у = ах + B 

#### Данные


![[Pasted image 20240611075819.png]]

#### Решение 
Параметры а и b по методу наименьших квадратов можно найти из системы уравнений:

![[Pasted image 20240611075842.png]]

где суммирование ведется по i от 1 до n, п = 8. Составим расчетную таблицу:

![[Pasted image 20240611075858.png]]

Получаем систему:

![[Pasted image 20240611075910.png]]

откуда находим а = 3,625, b = -4.146, то есть получаем функцию у = 3,625х4,146

Формулы для расчета параметров а и b:

![[Pasted image 20240611075926.png]]
