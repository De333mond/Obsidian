#Polykek/cheats/TOI #искусственный-интеллект 

## Классификация нейронных сетей и их свойства

Нейронная сеть представляет собой совокупность нейроподобных элементов, определенным образом соединенных друг с другом и с внешней средой с помощью связей, определяемых весовыми коэффициентами. В зависимости от функций, выполняемых нейронами в сети, можно выделить три их типа:
- входные нейроны, на которые подается вектор, кодирующий входное воздействие или образ внешней среды; в них обычно не осуществляется вычислительных процедур, а информация передается с входа на выход путем изменения их активации; 
- выходные нейроны, выходные значения которых представляют выходы нейронной сети; преобразования в них осуществляются по выражениям:

![[Pasted image 20240611081832.png]]

- промежуточные нейроны, составляющие основу нейронных сетей, преобразования в которых выполняются по тем же выражениям.

В большинстве нейронных моделей тип нейрона связан с его расположением в сети. Если нейрон имеет только выходные связи, то это входной нейрон, если наоборот — выходной нейрон. Однако возможен случай, когда выход топологически внутреннего нейрона рассматривается как часть выхода сети. В процессе функционирования сети осуществляется преобразование входного вектора в выходной, некоторая переработка информации. Конкретный вид выполняемого сетью преобразования данных обусловливается не только характеристиками нейроподобных элементов, но и особенностями ее архитектуры, а именно топологией межнейронных связей, выбором определенных подмножеств нейроподобных элементов для ввода и вывода информации, способами обучения сети, наличием или отсутствием конкуренции между нейронами, направлением и способами управления и синхронизации передачи информации между нейронами.

С точки зрения топологии можно выделить три основных типа нейронных сетей:
- полносвязные

		![[Pasted image 20240611081924.png]]

- полносвязные

		![[Pasted image 20240611081938.png]]

- слабосвязные (с локальными связями)

		![[Pasted image 20240611081951.png]]


В полносвязных нейронных сетях каждый нейрон передает свой выходной сигнал остальным нейронам, в том числе и самому себе. Все входные сигналы подаются всем нейронам. Выходными сигналами сети могут быть все или некоторые выходные сигналы нейронов после нескольких тактов функционирования сети.

В многослойных нейронных сетях нейроны объединяются в слои. Слой содержит совокупность нейронов с едиными входными сигналами. Число нейронов в слое может быть любым и не зависит от количества нейронов в других слоях. В общем случае сеть состоит из Q слоев, пронумерованных слева направо. Внешние входные сигналы подаются на входы нейронов входного слоя (его часто нумеруют как нулевой), а выходами сети являются выходные сигналы последнего слоя. Кроме входного и выходного слоев в многослойной нейронной сети есть один или несколько скрытых слоев. Связи от выходов нейронов некоторого слоя q к входам нейронов следующего слоя (q+1) называются последовательными.

В свою очередь, среди многослойных нейронных сетей выделяют следующие типы: 
1) Монотонные. 
	   Это частный случай слоистых сетей с дополнительными условиями на связи и нейроны. Каждый слой кроме последнего (выходного) разбит на два блока: возбуждающий и тормозящий. Связи между блоками тоже разделяются на тормозящие и возбуждающие. Если от нейронов блока А к нейронам блока В ведут только возбуждающие связи, то это означает, что любой выходной сигнал блока является монотонной неубывающей функцией любого выходного сигнала блока А Если же эти связи только тормозящие, то любой выходной сигнал блока В является невозрастающей функцией любого выходного сигнала блока А для нейронов монотонных сетей необходима монотонная зависимость выходного сигнала нейрона от параметров входных сигналов
		![[Pasted image 20240611082101.png]]
2) Сети без обратных связей. 
	   В таких сетях нейроны входного слоя получают входные сигналы, преобразуют их и передают нейронам первого скрытого слоя, и так далее вплоть до выходного, который выдает сигналы для интерпретатора и пользователя. Если не оговорено противное, то каждый выходной сигнал д-го слоя подастся на вход всех нейронов (q+1)-го слоя: однако возможен вариант соединения q-го слоя с произвольным (q+p)-M слоем. Среди многослойных сетей без обратных связей различают полносвязные (выход каждого нейрона q-ro слоя связан с входом каждого нейрона слоя) и частично полносвязные. Классическим вариантом слоистых сетей являются полносвязные сети прямого распространения (рисунок 4).
3) Сети с обратными связями. 
	   В сетях с обратными связями информация с последующих слоев передается на предыдущие. Среди них, в свою очередь, выделяют следующие:
		   - слоисто-циклические, отличающиеся тем, что слои замкнуты в кольцо. последний слой передает свои выходные сигналы первому; все слои равноправны и могут как получать входные сигналы, так и выдавать выходные; 
		   - слоисто-полносвязные состоят из слоев, каждый из которых представляет собой полносвязную сеть, а сигналы передаются как от слоя к слою, так и внутри слоя; в каждом слое цикл работы распадается на три части. прием сигналов с предыдущего слоя, обмен сигналами внутри слоя, выработка выходного сигнала и передача к последующему слою. 
		   - полносвязанно-слоистые, по своей структуре аналогичные слоистополносвязанным, но функционирующим по-другому: в них не разделяются фазы обмена внутри слоя и передачи следующему, на каждом такте нейроны всех слоев принимают сигналы от нейронов как своего слоя, так и последующих.

В качестве примера сетей с обратными связями на рисунок 5 представлены частично-рекуррентные сети Элмана и Жордана.

![[Pasted image 20240611082249.png]]

В слабосвязных нейронных сетях нейроны располагаются в узлах прямоугольной или гексагональной решетки Каждый нейрон связан с четырьмя (окрестность фон Неймана), шестью (окрестность Голея) или восемью (окрестность Мура) своими ближайшими соседями.

Известные нейронные сети можно разделить по типам структур нейронов на гомогенные (однородные) и гетерогенные. Гомогенные сети состоят из нейронов одного типа с единой функцией активации, а в гетерогенную сеть входят нейроны с различными функциями активации.

Существуют бинарные и аналоговые сети. Первые из них оперируют только двоичными сигналами, и выход каждого нейрона может принимать значение либо логического ноля (заторможенное состояние) либо логической единицы (возбужденное состояние).

Еще одна классификация делит нейронные сети на синхронные и асинхронные. В первом случае в каждый момент времени лишь один нейрон меняет свое состояние, во втором — состояние меняется сразу у целой группы нейронов, как правило, у всего слоя.

Сети можно классифицировать также по числу слоев. Теоретически число слоев и число нейронов в каждом слое может быть произвольным, однако фактически оно ограничено ресурсами компьютера.

Выбор структуры нейронной сети осуществляется в соответствии с особенностями и сложностью задачи. Если же задача не может быть сведена ни к одному из известных типов, приходится решать сложную проблему синтеза новой конфигурации. При этом необходимо руководствоваться следующими основными правилами:
- возможности сети возрастают с увеличением числа нейронов сети, плотности связей между ними и числом слоев;
- введение обратных связей наряду с увеличением возможностей сети поднимает вопрос о динамической устойчивости сети; 
- сложность алгоритмов функционирования сети, введение нескольких типов синапсов способствует усилению мощности нейронной сети

Вопрос о необходимых и достаточных свойствах сети для решения задач того или иного рода представляет собой целое направление нейрокомпьютерной науки. В большинстве случаев оптимальный вариант получается на основе интуитивного подбора.

Многие задачи распознавания образов (зрительных, речевых), выполнения функциональных преобразований при обработке сигналов, управления, прогнозирования, идентификации сложных систем, сводятся к следующей математической постановке. Необходимо построить такое отображение 𝑋 → 𝑌, чтобы на каждый возможный входной сигнал X формировался правильный выходной сигнал Y. Отображение задается конечным набором пар (<вход>, <известный выход>). Число этих пар (обучающих примеров) существенно меньше общего числа возможных сочетаний значений входных и выходных сигналов. Совокупность всех обучающих примеров носит название обучающей выборки.

В задачах распознавания образов Х — некоторое представление образа (изображение, вектор), Y— номер класса, к которому принадлежит входной образ. 

В задачах управления Х — набор контролируемых параметров управляемого объекта, Y— код, определяющий управляющее воздействие, соответствующее текущим значениям контролируемых параметров. 

В задачах прогнозирования в качестве входных сигналов используются временные ряды, представляющие значения контролируемых переменных на некотором интервале времени. Выходной сигнал — множество переменных, которое является подмножеством переменных входного сигнала. 

При идентификации Х и Y представляют входные и выходные сигналы системы соответственно.

Вообще говоря, большая часть прикладных задач может быть сведена к реализации некоторого сложного функционального многомерного преобразования. В результате отображения 𝑋 → 𝑌 необходимо обеспечить формирование правильных выходных сигналов в соответствии:

- со всеми примерами обучающей выборки; 
- со всеми возможными входными сигналами, которые не вошли в обучающую выборку.

Второе требование в значительной степени усложняет задачу формирования обучающей выборки В общем виде эта задача в настоящее время еще не решена, однако во всех известных случаях может быть найдено частное решение

## Теорема Колмогорова—Арнольда

Построить многомерное отображение Х Y — это значит представить его с помощью математических операций над не более, чем двумя переменными.

В результате многолетней научной полемики между А. Н. Колмогоровым и В. И. Арнольдом был получен ряд важных теоретических результатов, опровергающих тезис непредставимости функции многих переменных функциями меньшего числа переменных:

- теорема о возможности представления непрерывных функций нескольких переменных суперпозициями непрерывных функций меньшего числа переменных (1956г) 
- теорема о представлении любой непрерывной функции трех переменных в виде суммы функций не более двух переменных (1957г) 
- теорема о представлении непрерывных функций нескольких переменных в виде суперпозиций непрерывных функций одного переменного и сложения (1957г)

## Работа Хехт-Нильсена

Теорема Хехт-Нильсена о представлении непрерывных функций нескольких переменных в виде суперпозиций непрерывных функций одного переменного и сложения доказывает представимость функции многих переменных достаточно общего вида с помощью двухслойной нейронной сети с прямыми полными связями с п нейронами входного слоя, (2n+1) нейронами скрытого слоя с заранее известными ограниченными функциями активации (например, сигмоидальными) и т нейронами выходного слоя с неизвестными функциями активации

Теорема, таким образом, в неконструктивной форме доказывает решаемость задачи представления функции произвольного вида на нейронной сети и указывает для каждой задачи минимальные числа нейронов сети, необходимых для ее решения

### Следствия из теоремы Колмогорова—Арнольда — Хехт-Нильсена

1. Из теоремы Хехт-Нильсена следует представимость любой многомерной функции нескольких переменных с помощью нейронной сети фиксированной размерности Неизвестными остаются следующие характеристики функций активации нейронов 
	- ограничения области значений (координаты асимптот) сигмоидальных функций активации нейронов скрытого слоя, 
	- наклон сигмоидальных функций активации, 
	- вид функций активации нейронов выходного слоя 
	
	На практике требования теоремы Хехт-Нильсена к функциям активации удовлетворяются следующим образом В нейронных сетях как для первого (скрытого), так и для второго (выходного) слоя используют сигмоидальные передаточные функции с настраиваемыми параметрами То есть в процессе обучения индивидуально для каждого нейрона задается максимальное и минимальное значение, а также наклон сигмоидальной функции
2. Для любого множества пар (Xk , Yk ) (где Yk— скаляр) существует двухслойная однородная (с одинаковыми функциями активации) нейронная сеть первого порядка с последовательными связями и с конечным числом нейронов, которая выполняет отображение 𝑋 → 𝑌, выдавая на каждый входной сигнал Xk правильный выходной сигнал Yk Нейроны в такой двухслойной нейронной сети должны иметь сигмоидальные передаточные функции
   
   К сожалению, эта теорема не конструктивна. В ней не заложена методика определения числа нейронов в сети для некоторой конкретной обучающей выборки.
   
   Для многих задач единичной размерности выходного сигнала недостаточно Необходимо иметь возможность строить с помощью нейронных сетей функции 𝑋 → 𝑌, где Y имеет произвольную размерность Следующее утверждение является теоретической основой для построения таких функций на базе однородных нейронных сетей.
   
   Утверждение. для любого множества пар входных-выходных векторов произвольной размерности {( Xk , Yk ), k = 1 ...N} существует однородная двухслойная нейронная сеть с последовательными связями, с сигмоидальными передаточными функциями и с конечным числом нейронов, которая для каждого входного вектора формирует соответствующий ему выходной вектор Yk
   
   Таким образом, для представления многомерных функций многих переменных может быть использована однородная двухслойная нейронная сеть с сигмоидальными передаточными функциями.
   
   для оценки числа нейронов с скрытых слоях однородных нейронных сетей можно воспользоваться формулой для оценки необходимого числа синаптических весов 𝐿𝑤 в многослойной сети с сигмоидальными передаточными функциями:
   
   ![[Pasted image 20240611082723.png]]
   
   де n — размерность входного сигнала, m — размерность выходного сигнала, N — число элементов обучающей выборки. 
   
   Оценив необходимое число весов, можно рассчитать число нейронов в скрытых слоях. Например, для двухслойной сети это число составит:
   ![[Pasted image 20240611082750.png]]
   Известны и другие формулы для оценки, например: 
   `2(𝑛 + 𝐿 + 𝑚) ≤ 𝑁 ≤ 10(𝑛 + 𝐿 + 𝑚)`
   
   `𝑁 / 10 − 𝑛 − 𝑚 ≤ 𝐿 ≤ 𝑁 2 − 𝑛 − 𝑚`
   
   Точно так же можно рассчитать число нейронов в сетях с большим числом слоев. 
   
   Отметим, что отечественному читателю приведенные результаты известны в более фрагментарной форме — в виде так называемой теоремы о полноте.

## Теорема о полноте

Любая непрерывная функция на замкнутом ограниченном множестве может быть равномерно приближена функциями, вычисляемыми нейронными сетями, если функция активации нейрона Дважды непрерывно Дифференцируема и непрерывна.

Таким образом, нейронные сети являются универсальными структурами, позволяющими реализовать любой вычислительный алгоритм

### Постановка и возможные пути решения задачи обучения нейронных сетей

Очевидно, что процесс функционирования нейронной сети, сущность действий, которые она способна выполнять, зависит от величин синаптических связей. Поэтому, задавшись определенной структурой сети, соответствующей какой-либо задаче, необходимо найти оптимальные значения всех переменных весовых коэффициентов (некоторые синаптические связи могут быть постоянными).

Этот этап называется обучением нейронной сети, и от того, насколько качественно он будет выполнен, зависит способность сети решать поставленные перед ней проблемы во время функционирования.

В процессе функционирования нейронная сеть формирует выходной сигнал Y в соответствии с входным сигналом Х, реализуя некоторую функцию g: Y = g(Х). Если архитектура сети задана, то вид функции д определяется значениями синаптических весов и смещений сети. Обозначим через G множество всех возможных функций д, соответствующих заданной архитектуре сети.

Пусть решение некоторой задачи есть функция r. Y = r(Х), заданная парами входных-выходных данных (Y1 , X1) , …, (Yk , Xk ), для которых Yk = r(Xk ), k ≈ 1. N. Е— функция ошибки (функционал качества), показывающая для каждой из функций д степень близости к г.

Решить поставленную задачу с помощью нейронной сети заданной архитектуры — это значит построить (синтезировать) функцию g ∈ G, подобрав параметры нейронов (синаптические веса и смещения) таким образом, чтобы функционал качества обращался в оптимум для всех пар (Yk , Xk ).

Таким образом, задача обучения нейронной сети определяется совокупностью пяти компонентов:

Обучение состоит в поиске (синтезе) функции G, оптимальной по E. Оно требует длительных вычислений и представляет собой итерационную процедуру. Число итераций может составлять от 103 до 108 . На каждой итерации происходит уменьшение функции ошибки.

Функция Е может иметь произвольный вид. Если выбраны множество обучающих примеров и способ вычисления функции ошибки, обучение нейронной сети превращается в задачу многомерной оптимизации, для решения которой могут быть использованы следующие методы:

- локальной оптимизации с вычислением частных производных первого порядка; 
- локальной оптимизации с вычислением частных производных первого и второго порядка; 
- стохастической оптимизации; 
- глобальной оптимизации.

К первой группе относятся: градиентный метод (наискорейшего спуска); методы с одномерной и двумерной оптимизацией целевой функции в направлении антиградиента; метод сопряженных градиентов; методы, учитывающие направление антиградиента на нескольких шагах алгоритма.

Ко второй группе относятся метод Ньютона, методы оптимизации с разреженными матрицами Гессе, квазиньютоновские методы, метод Гаусса—Ньютона, метод Левенберга—Маркардта.

Стохастическими методами являются: поиск в случайном направлении, имитация отжига, метод Монте-Карло (численный метод статистических испытаний).

Задачи глобальной оптимизации решаются с помощью перебора значений переменных, от которых зависит целевая функция.