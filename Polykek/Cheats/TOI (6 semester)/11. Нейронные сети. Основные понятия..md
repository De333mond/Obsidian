#Polykek/cheats/TOI #искусственный-интеллект 

### Структура искусственного нейрона

Нейрон является составной частью нейронной сети. На рис. 1 показана его структура. Он состоит из элементов трех типов: умножителей (синапсов), сумматора и нелинейного преобразователя. Назначение синапсов –это осуществление связи между нейронами, умножение входного сигнала на число, характеризующее силу связи, (вес синапса). Сумматор выполняет сложение сигналов, которые поступают по синаптическим связям от других нейронов, и внешних входных сигналов. Нелинейный преобразователь реализует выхода сумматора – нелинейную функцию одного аргумента. Эта функция называется функцией активации или передаточной функцией нейрона.

![[Pasted image 20240611081319.png]]

### Математическая модель нейрона

Нейрон в целом реализует скалярную функцию векторного аргумента. Математическая модель нейрона представлена на рис. 2:

![[Pasted image 20240611081342.png]]

где w i, - вес (weight) синапса, i = 1...n; b - значение смещения (bias); s - результат суммирования (sum); x, - компонент входного вектора (входной сигнал), i = 1... n ; у - выходной сигнал нейрона; n - число входов нейрона; f - нелинейное преобразование (функция активации).

Входной сигнал, весовые коэффициенты и смещение могут принимать действительные значения. Во многих практических задачах - лишь некоторые фиксированные значения. Выход (у) зависит от вида функции активации и может быть как действительным, так и целым. Синаптические связи с положительными весами называют возбуждающими, с отрицательными весами - тормозящими. Представленный вычислительный элемент можно считать упрощенной математической моделью биологических нейронов. Для разграничения различий нейронов биологических и искусственных, вторые иногда называют нейроноподобными элементами или формальными нейронами.

### Функции активации. Достоинства и недостатки

Функция активации – это фрагмент программного кода, добавляемый в искусственную Нейронную сеть (Neural Network) в качестве помощи для изучения сложных закономерностей данных. Функция активации, в сравнении с нейронами нашего мозга, решает, что должно быть передано в следующий нейрон. Она принимает выходной сигнал из предыдущей ячейки и преобразует его в некоторую форму, которую можно использовать в качестве входных данных для следующей ячейки

Входной сигнал (s) нелинейный преобразователь заменяет выходным сигналом f(s), который представляет собой выход у нейрона.

Кроме биологического сходства, нелинейные функции активации также поддерживают значение выходного сигнала нейрона, который ограничен определенным пределом в соответствии с установленными требованиями.

Значение смещения (b), не ограничиваясь определенным пределом, может быть очень большим, в частности в случае глубоких нейронных сетей, имеющих миллионы параметров. Это может привести к вычислительным проблемам.

Примеры активационных функций представлены в табл. 1. и на рис. 3.

![[Pasted image 20240611081434.png]]

![[Pasted image 20240611081445.png]]

Среди наиболее распространенных можно выделить нелинейную функцию активации с насыщением (рис. 5), так называемую логистическую функцию или сигмоид (функция S-образного вида, формула которой представлена на рис. 4):

![[Pasted image 20240611081455.png]]

![[Pasted image 20240611081502.png]]

При уменьшении а сигмоид становится более пологим, в пределе при а = 0 преобразовываясь в горизонтальную линию на уровне 0,5, при увеличении а сигмоид приближается к виду функции единичного скачка с порогом 0. Выражения для сигмоида дает понять, что выходное значение нейрона лежит в диапазоне (0, 1). Простое выражение для ее производной является одним из ценных свойств сигмоидальной функции (рис. 6). Его применение будет рассмотрено далее:

![[Pasted image 20240611081512.png]]

ажен факт, что сигмоидальная функция дифференцируема на всей оси абсцисс. Это используется в некоторых алгоритмах обучения. Также она способна усиливать слабые сигналы лучше, чем большие, и исключает насыщение от больших сигналов по причине того, что они соответствуют областям аргументов, в которых сигмоид имеет пологий наклон. 

#### Рассмотрим её преимущества.
Важно, что сигмоида не линейна по своей природе. Результат комбинации набора таких функций имеет нелинейный характер. 

Это дает возможность строить сеть из нескольких слоев. Она не является бинарной, в отличие от ступенчатой функции и это считается её главным преимуществом. Для сигмоиды также характерен гладкий градиент \[3, 4\].

В диапазоне значений X (-2;2) значения Y изменяется очень быстро. Это приводит к тому, что любое малое изменение значения X в этой области приводит к существенному изменению значения Y. Такое поведение функции указывает на то, что Y имеет тенденцию стремится к одному из краев диапазона. Еще одно преимущество сигмоиды над линейной функцией заключается в следующем. Логическая функция имеет фиксированный диапазон значений функции — \[0,1\], тогда как линейная функция изменяется в пределах (-∞, ∞). Данной свойство сигмоиды очень полезно потому, что оно не приводит к ошибкам при больших значениях порога активации.

Сигмоида является одной из самых часто применяемых активационных функций в нейросетях, но она имеет недостатки, на которые стоит обратить внимание. При приближении к границам диапазона, значения Y начинают слабо реагировать на изменения в X. Из этого следует, что градиент в таких областях имеет малые значения.

При этом значение градиента мало и стремится к 0 Нейросеть прекращает обучаться совсем или делает это очень медленно. Всё же существует множество способов решения данного вопроса. Несмотря на свои недостатки эта функция очень востребована, поэтому часто применяется для решения задач классификации.

<u>Линейная функция</u> (рис. 7) имеет два линейных участка, где функция активации равна минимально допустимому и максимально допустимому значению. Эта активационная функция выдает целый спектр значений, а не только бинарный ответ. Кроме этого, появлется возможность объединения нескольких нейронов вместе и при условии, что более одного нейрона активировано, решение принимается на основе максимального значения. Как и ранее рассмотренная логическая функция данная функция имеет свои недостатки.

![[Pasted image 20240611081640.png]]

Производная от функции по x равна с, поэтому градиент никак не связан с Х. Градиент является постоянным вектором, а спуск производится по постоянному градиенту. При получении ошибочного ответа, дообучение с помощью метода обратного распространения ошибки, тоже будет постоянно, и не зависимо от изменения на входе значения ∆(x), что не является хорошим знаком. Также существует и другая проблема.

Рассмотрим связанные слои, каждый из которых активируется линейной функцией. Значение с этой функции поступает в следующий слой в качестве входа. Второй слой считает взвешенную сумму на своих входах и устанавливает нейроны в зависимости от другой линейной активационной функции. Не имеет значения, сколько слоев было использовано, при условии, что, если все они линейные, то и финальная функция активации будет линейна. Это дает понять, что два слоя (или N слоев) могут быть заменены одним слоем.

Еще одна часто применяемая активационная функция — гиперболический тангенс (рис. 8).

![[Pasted image 20240611081657.png]]

Данная функция имеет те же характеристики, что и сигмоида, рассмотренная ранее, в частности она так же имеет не линейный характер и хорошо подходит для комбинации слоёв. При этом диапазон значений функции лежит в диапазоне (-1, 1). Узкий диапазон значений откидывает вероятность перегрузки активационной функции. Также стоит отметить, что градиент тангенциальной функции больше, чем у сигмоиды (производная круче). Решение в выборе функции связано с требованиями к амплитуде градиента. Гиперболическому тангенсу, как и сигмоиде, свойственна проблема исчезновения градиента.

### Заключение

В данной лекции мы изучили понятие «нейронная сеть», рассмотрели структуру нейрона, а также его математическую модель и функции активации. Каждая из рассмотренных функций имеет свои отличительные свойства, преимущества и недостатки. Не одна из функции не является универсальной. Точно сказать, когда следует использовать сигмоиду или гиперболический тангенс сложно. Зная некоторые характеристики функции, которую требуется аппроксимировать, следует выбирать такую активационную функцию, которая аппроксимирует искомую функцию максимально точно и приведёт к быстрому обучению.

Например, сигмоида хорошо применима в задачах классификации, а аппроксимацию классифицирующей функции комбинацией сигмоид можно провести легче, чем применяя ReLu.

Необходимо использовать ту функцию, при которой процесс обучения и сходимость будут быстрее. При этом возможно применение собственных функций.
