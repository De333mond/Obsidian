#Polykek/cheats/TOI #искусственный-интеллект 

Дерево решений – один из алгоритмов в арсенале Машинного обучения. Один из самых известных его представителей, основная идея алгоритма – классификация данных на основе генерации логических условий. Имея входные данные и итоговый результат, алгоритм пытается так подобрать условия (классические if else) чтобы в как можно большем количестве случаев выдавать правильный результат.

![[Pasted image 20240611080028.png]]

### Построение дерева принятия решений

В ходе построения бинарного дерева решений используется понятие энтропии (как меры хаотичности). 

Энтропия Шеннона определяется для системы с N возможными состояниями следующим образом:

![[Pasted image 20240611080051.png]]

Где Pi - вероятности нахождения системы в i-ом состоянии. Это очень важное понятие, используемое в физике, теории информации и других областях. Чем выше энтропия, тем менее упорядочена система и наоборот

![[Pasted image 20240611080203.png]]

Здесь 9 синих шариков и 11 желтых. Ниже приведены удельные вероятности, характеризующие соответствие какой-либо координате синего и желтого шара соответственно:

![[Pasted image 20240611080223.png]]

Теперь посмотрим, как изменится энтропия, если разбить шарики на две группы - с координатой меньше либо равной 12 и больше 12

![[Pasted image 20240611080307.png]]

В левой подгруппе оказалось 13 шаров, из которых 8 синих и 5 желтых.

![[Pasted image 20240611080318.png]]

Энтропия этой группы равна: В правой группе оказалось 7 шаров, из которых 1 синий и 6 желтых. Энтропия правой группы равна:

![[Pasted image 20240611080342.png]]

Энтропия уменьшилась в обеих группах по сравнению с начальным состоянием, хоть в левой и не сильно. Поскольку энтропия - по сути степень хаоса (или неопределенности) в системе, уменьшение энтропии называют приростом информации. Формально прирост информации (information gain, IG) при разбиении выборки по признаку Q (в нашем примере это признак X<=12) определяется как

![[Pasted image 20240611080434.png]]

![[Pasted image 20240611080444.png]]

Как видно значение равное 12-ти дает лучший результат. Поэтому выбирается именно оно. Используя данный подход продолжим деление шариков на группы до тех пор, пока в каждой группе шарики не будут одного цвета.

![[Pasted image 20240611080504.png]]

Для правой группы потребовалось всего одно дополнительное разбиение по признаку "координата меньше либо равна 18", для левой - еще три. Очевидно, энтропия группы с шариками одного цвета равна0(1 = 0), что соответствует представлению, что группа шариков одного цвета - упорядоченная.

В итоге мы построили дерево решений, предсказывающее цвет шарика по его координате.

## Алгоритм построения дерева принятия решений

В общих чертах, идею алгоритма построения дерева принятия решений можно описать следующим образом:

![[Pasted image 20240611080537.png]]

Главным достоинством является, получаемая в результате, древовидная структура предикатов, которая позволяет интерпретировать результаты классификации (хотя в силу своей «жадности», описанный алгоритм, не всегда позволяет обеспечить оптимальность дерева в целом). 

Важным вопросом описанного алгоритма является критерий остановки при построении дерева. Если прекращать построение дерева по достижении нулевой энтропии, то можно полностью подогнать дерево принятия решений под обучающую выборку данных, но это не всегда эффективно с практической точки зрения (полученное дерево является переобученным). 

Одним из возможных критериев остановки может быть небольшое значение AS. Но при таком подходе, всё же, невозможно дать универсальный совет: при каких значениях AS следует прекращать построение дерева.

