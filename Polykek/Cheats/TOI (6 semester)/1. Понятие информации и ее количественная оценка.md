#Polykek/cheats/TOI #понятие-информации

## Понятие информации 
Термин "**информация**" происходит от латинского слова "Informatiо"- разъяснение, изложение, осведомленность. На настоящий момент не существует точного определения для этого слова, но можно считать, что этот термин в начальном представлении является общим понятием, означающим некоторые сведения, совокупность данных, знаний и т.д. В течение многих веков понятие информации не раз претерпевало изменения, то расширяя, то предельно сужая свои границы. 

Существует несколько определений понятия «информация». 
* (Н. Винер) – информация означает содержание, полученное из внешнего мира в процессе приспособления к ней нас и наших органов чувств. 
* (К. Шеннон) – это мера той неопределенности, которая исчезает после получения сведений о системе. 
* (Н.М. Амосов) – информация это нечто, что может передаваться от одной системы к другой, в результате чего изменяются обе системы (т.е. когда две системы взаимодействуют между собой: одна система отдает, другая чтото принимает) т.е. это нечто и есть «информация». 

### Можно привести более общее определение: 

> [!info] 
>  **Информация** – это совокупность сведений о всевозможных явлениях, объектах и предметах, привносящих новые знания о них. Многочисленность определений информации связана с одной стороны с широтой этого понятия, с другой со свойствами присущими информации. 


## Свойства информации 
Свойства информации: 
1. Информация приносит знания об окружающем мире, которых в рассматриваемой точке пространства не было до получения информации. 
2. Информация не материальна, но проявляется в форме материальных носителей – дискретных знаков и сигналов. 
3. Информация может быть заключена и в знаках и в их взаимном расположении. 
4. Знаки и сигналы несут информацию только для получателя, способного их распознать. 

Использование информации для целей управления ставит вопросы ее количественной оценки. 

## Количественная оценка информации
В качестве основной характеристики сообщения теория информации принимает величину, называемую количеством информации. Это понятие не затрагивает смысла и важности передаваемого сообщения, а связано со степенью его неопределенности. 

Попытки количественного измерения информации предпринимались неоднократно. Первые отчетливые предложения об общих способах измерения количества информации были сделаны Р. Фишером (1921 г.) в процессе решения вопросов математической статистики. Проблемами хранения информации, передачи ее по каналам связи и задачами определения количества информации занимались Р. Хартли (1928 г.) и X. Найквист (1924 г.). Р. Хартли заложил основы теории информации – он предложил логарифмическую меру информации. Процесс получения информации рассматривался как выбор одного сообщения из конечного наперед заданного множества из N равновероятных сообщений, а количество 
информации I, содержащееся в выбранном сообщении, определялась формулой: 

> [!hint] 
> I = log2 N  

Формула Хартли подходит лишь для идеальных систем. В действительности же сообщения не являются равновероятными. 

В 1948 г. американский инженер и математик Клод Шеннон сформулировал основы математической теории информации. Он исследовал случайные процессы и явления, для которых характерна неопределенность исхода (да, нет). Под информацией понимались не любые сведения (сообщения), а лишь те, которые снимают полностью или уменьшают существовавшую до их получения неопределенность. Согласно теории К. Шеннона, **информация — это снятая неопределенность.**

Неопределенность существует тогда, когда может произойти одно из нескольких событий. Количество информации, получаемой в результате снятия неопределенности, вычисляется по формуле, называемой формулой Шеннона:

![[Pasted image 20240611053833.png]]

- где I — количество информации; 
- Pi — вероятность события, 1≤i≤m; 
- m — число всех возможных событий. Формула определения количества информации учитывает возможную неодинаковую вероятность сообщений.

Таким образом, количественной оценкой информации выступает ее объем — количество символов в сообщении. Физический смысл количества информации (I) – это ёмкость «тары» для транспортировки и хранения смысла.

## Классификация информации

Разработано множество подходов к классификации информации. В их основу положены различные признаки и особенности информации. Например, информацию соответственно **восприятию органами чувств** называют визуальной, аудиальной, аудиовизуальной, тактильной, вкусовой. По **степени изменчивости** информация подразделяется на <u>постоянную, переменную и смешанную</u>. По стадии использования — <u>первичную и вторичную</u>. 

Среди внутренних свойств информации важнейшими являются <u>количество информации, ее внутренняя организация и структура</u>. По способу внутренней организации информация делится на две группы: <u>данные</u>, или простой, логически неупорядоченный набор сведений, и логически упорядоченный, организованный <u>набор данных</u>. 

К свойствам информации, связанным с ее хранением, относятся: 
- живучесть — свойство сохранять качество с течением времени; 
- актуальность — степень соответствия информации текущему моменту времени. 

Существует три основных способа измерения информации: 
- объемный; 
- алгоритмический; 
- энтропийный.

<u>Объемный способ </u> измерения чувствителен к форме представления информации. Например, число может быть записано как 22, XXII, двадцать два. Во всех трех видах записи числа количество символов будет разным. Количественной оценкой информации выступает ее объем — количество символов в сообщении. 

<u>Алгоритмический способ</u> оценки информации предлагается алгоритмической теорией информации, которая разработана в конце 1960-х годов А. Колмогоровым, Р. Соломоновым и Г. Хайтиным. Любое сообщение может быть оценено количественной характеристикой, отражающей размер программы, которая позволяет ее произвести. За основу принимается последовательность из нулей и единиц как длина самой короткой программы, которая может произвести эту последовательность.

При энтропийном подходе (энтропия — мера неопределенности) количество информации определяется формулой Шеннона. Им же впервые введена единица измерения количества информации бит (binary digital — 4 бинарная единица). Она соответствует минимальному объему информации, полученной в виде ответа «да» или «нет», в двоичной системе 1 или 0. В данном случае происходят два равновероятностных события:

![[Pasted image 20240611060206.png]]

## Модели Маркова нулевого и первого порядка для оценки энтропии текста

Марков Андрей Андреевич (1856-1922г.) — русский математик, специалист по теории чисел, теории вероятностей и математическому анализу. 

Це́пь Ма́ркова — последовательность случайных событий с конечным или счётным числом исходов, где вероятность наступления каждого события зависит только от состояния, достигнутого в предыдущем событии. Цепь Маркова – это распространенный и довольно простой способ моделирования случайных событий. Простыми словами, это всего-лишь описание вероятностей перехода системы из одного состояния в другое.

Рассмотрим смысл алгоритма на простом примере: 

  Допустим, у нас есть событие «Взять зонт», которое всегда идёт только после события «Идёт дождь». Даже если сейчас середина декабря, кругом снег, но внезапно случилась оттепель и пошёл дождь — следом за ним по этой логике будет событие «Взять зонт». 
 
  Погода не может кардинально поменяться за один день. На это влияет множество факторов. Завтрашняя погода напрямую зависит от текущей и т. д. Таким образом, для того чтобы предсказывать погоду, вы на протяжении нескольких лет собираете данные и приходите к выводу, что после пасмурного дня вероятность солнечного равна 0,25. Логично предположить, что вероятность двух пасмурных дней подряд равна 0,75, так как мы имеем всего два возможных погодных условия. Этот пример представлен на рис. 1.

![[Pasted image 20240611060329.png]]

А теперь рассмотрим, как найти энтропию марковского процесса на примере задачи:

Какова вероятность того, что при заданных предыдущих символах si1, si2, … , sim следующим символом будет si? В математике эта условная вероятность обозначается следующим образом:

![[Pasted image 20240611060552.png]]

Отметим, что последовательность символов будет выглядеть так:

![[Pasted image 20240611060615.png]]

Для марковского процесса нулевого порядка вероятности зависят только от символа si и для них можно использовать прежнее обозначение pi. Для марковского источника первого порядка существенны частоты пар символов алфавита. Если следование символов алфавита зависимо (например, во французском языке после буквы «q» почти всегда следует «u») количество информации, которую несёт последовательность таких символов (а, следовательно, и энтропия) очевидно меньше. Для учёта таких фактов используется условная энтропия.

<u>Условной энтропией первого порядка</u> (аналогично для Марковской модели первого порядка) называется энтропия для алфавита, где известны вероятности появления одной буквы после другой (т.е. вероятности двухбуквенных сочетаний).

![[Pasted image 20240611060658.png]]

С формальной точки зрения цепь Маркова представляет собой «автомат», который генерирует вероятности, в зависимости от входных данных. По факту цепь Маркова будет иметь вид матрицы, где будет прописана вероятность наступления следующего события, отталкиваясь от нынешнего. Главное, что нужно знать: марковские цепи генерируют вероятность перехода из одного состояния, а не нескольких. То есть сегодня либо дождь, либо солнечно, а цепь Маркова не может учитывать, что солнце и дождь будут одновременно.

Генерация текста происходит по тому же принципу. Алгоритм цепи Маркова предсказывает вероятность использования того или иного слова.